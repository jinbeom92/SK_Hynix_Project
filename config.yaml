# =============================================================================
# Example YAML configuration for HDN/SVTR training with pixel-level loss.
#
# This configuration uses the default dataset layout where sinograms have
# shape [U, A, D] and voxels [X, Y, D]【511144253987820†L17-L23】.  Each item
# returned by the dataset corresponds to a single depth slice: a sinogram
# slice of size [U, A] and a voxel slice of size [1, X, Y]【511144253987820†L75-L99】.
#
# The ``losses.weights`` section includes a new ``l1`` weight which controls
# the pixel-wise L1 loss.  During training, this L1 term is combined with
# structural similarity (SSIM) and peak signal-to-noise ratio (PSNR) to
# encourage accurate and stable reconstructions.  All weights are
# automatically normalised to sum to 1.0 inside the training script.
#
# Set ``cheat2d.enabled`` to true or false depending on whether you want to
# inject ground-truth voxel features during training.  When evaluating
# (train_mode=False), cheat injection is disabled to avoid leaking GT
# information【511144253987820†L75-L99】.
#
# Forward-projection consistency can be enabled by setting ``fp`` to a
# positive value; this will use the differentiable radon transform to
# compare the predicted reconstruction's sinogram with the ground truth.
#
# References:
# - ASTRA forward projection algorithm takes a projector and a volume
#   and returns projection data【34246792875788†L36-L40】.
# - VAMToolbox’s FBP stage introduces negative intensities which we
#   clamp to [0,1] in the decoder【508774924062640†L39-L44】.
# =============================================================================
data:
  root: dataset
  sino_glob: "sino/*_sino.npy"
  voxel_glob: "voxel/*_voxel.npy"
  resample_u: false

model:
  enc1:   { base: 64, depth: 8 }
  enc2:   { base: 64, depth: 8 }
  align:  { out_ch: 96, depth: 8, interp_mode: bilinear }
  cheat2d: { enabled: true, base: 64, depth: 8 }
  fusion: { out_ch: 128 }
  dec:    { mid_ch: 128, depth: 8 }
  cheat_scale: 0.02

train:
  seed: 1337
  files_per_group: 50
  epochs_per_group: 0
  epochs: 5
  batch_size: 5
  lr: 1.0e-5
  weight_decay: 1.0e-5
  optimizer: "adamw"
  grad_accum_steps: 1
  amp: true
  amp_dtype: "auto"
  compile: false
  num_workers: 2
  ckpt_dir: "results"
  flush_every: 100
  empty_cache_every: 10
  grad_clip: 1.0
  train_ratio: 0.7

losses:
  weights:
    l1:     0.2
    L2:     0.2
    ssim:   0.2   # structural similarity index weight
    psnr:   0.1   # PSNR (converted to loss via psnr_ref) weight
    band:   0.5
    energy: 0.5
    ver:    0.5
    ipdr:   0.5
    tv:     0.0
    fp:     0.1
  band_low:  0.0
  band_high: 1.0
  ver_thr:   0.1